%------------------------------------------------------------------------------
\chapter{Conclusions and Future work}
\label{ch:conclusions}

\section{Conclusions}

We have presented a physically-based rendering algorithm for flames, the model uses the Radiative Transport Equation, which describes heat transfer in real flames; the variations produced when using different types of fuels, and the effects of the visual adaptation processes in the human eye.
The model carries all the limitations inherent to ray-tracing techniques, such as large computational costs, with render times per frame varying from a few seconds in low quality settings, to several hours for converged images.

The method assumes a spectral representation of the world, however even if we were to compute the final result using the spectral domain, a final transformation to RGB space and gamma correction is always needed in order to display the image on a monitor.
The RGB conversion incurs in an unavoidable loss of information, moreover in order to achieve practical rendering times, the transformation is often computed earlier.

Several authors such as~\cite{Hong:2007},~\cite{Horvath:2009} and~\cite{Jamriska:2015} use simpler rendering techniques than the one discussed in this report~\cite{Pegoraro:2006}.
Nevertheless, the results obtained by Pegoraro and Parker appear to be surpassed by the aforementioned authors; superior input data seems to be the cause of the disparity in quality.
This effect is more evident in the case of~\cite{Hong:2007}, where data with added ``vorticity" gives the impression of a significant output improvement, even if it is not based on any physical phenomena, .

Although the method is physically based, Pegoraro and Parker did not conduct any formal analysis of the validity of their model.
Such study would require the measurement of the physical properties of a real flame, such as densities, temperatures and spectral emissions; and comparing the data with values computed using the authors' technique.

\section{Future Work}

An interesting area of future work involves automatically setting the shader parameters for a given scene.
There are at least two paths that be used to give a good parameter estimation if we have captured data.
The methods are, gradient descent using image derivatives or reconstructing the spectrum which produced the cameras RGB responses.
Both techniques will be explained in greater detail in the Sections~\ref{sec:image_differences} and~\ref{sec:spectrum_reconstruction}.

A common line of work in the computer graphics community is the development of importance sampling for BxDF rendering models~\cite{Lawrence:2004},~\cite{Ou:2012} and~\cite{Wang:2014}.
The core idea is to sample more often the values that contribute more to the final image, by means of a biased distribution instead of an uniform sampling scheme.
In our implementation the emitted radiance $L_e$ is uniformly sampled, and in the general case the $\omegam_i$ directions for in-scattering light $L_i$ would have been naively sampled as well.
In order to design a ``good" biased distribution, prior knowledge of the location of the important regions is needed. 
For $L_e$ and $L_i$ we have some intuitive insight on were such regions would lie, samples whose origin is close to the interest point should have greater contributions, as the influence of further ones will be diminished by the light distance falloff effect.
Another factor which must be considered is the relative intensity emitted at source by a sample, as it is expected that brighter points will have larger contributions. 

\subsection{Image differences}
\label{sec:image_differences}

Given a ground truth captured image $I_f$, we would like to transform an image $I_0$, rendered with our method, so that it resembles $I_f$.
Formally the transformation is defined as

\begin{equation}
I_{i+1} = t(I_i,~d(I_i,~I_f)),
\end{equation}

where $I_i$ is the image in the $i^{th}$ step, $d$ is an image difference function, and $t$ is a function that transform $I_i$ in the direction of the gradient given by $d$.
There are several terms in the aforementioned equation that can be explored in more detail.
A number of image difference filters have been proposed, such as the Sobel and the Scharr filters.
The transformation function $t$ must convert from image derivatives space to the physical parameter space that our shaders use, e.g temperature and density.
The simplest technique to compute the next step in the parameter space would be gradient descent.
As there are several parameters to explore, and their behaviour is non-linear, it is reasonable to expect that the function will have a number of local minima, thus requiring more advanced optimization methods, such as genetic programming~\cite{Dobashi:2012}.

\subsection{Spectrum reconstruction}
\label{sec:spectrum_reconstruction}

In a captured image $I_f$, each pixel stores the RGB values that were measured by the sensor at that position.
When the camera is exposed to a spectral signal, the spectrum is collapsed to RGB space using the sensor spectral sensitivity curves for each color, the curves are shown in Figure~\ref{fig:camera_sensitivity}.
In continuous form, the collapsing of the signal is driven by

\begin{figure}[htbp!]
\centering
\includegraphics[width=0.8\textwidth]{img/camera_sensitivity}
	\caption{Spectral sensitivity curves for the sensors in our cameras.}
	\label{fig:camera_sensitivity}
\end{figure}

\begin{equation}
\begin{split}
r &= \int i(\lambda) s_r(\lambda) d \lambda, \\
g &= \int i(\lambda) s_g(\lambda) d \lambda, \\
b &= \int i(\lambda) s_b(\lambda) d \lambda,
\end{split}
\label{eq:spectrum_collapse_cont}
\end{equation}

where $r$, $g$ and $b$ are the output coefficients, $i(\lambda)$ is the input spectrum, $s_r(\lambda)$, $s_g(\lambda)$ and $s_b(\lambda)$ are the spectral sensitivity curves and $\lambda$ is a wavelength number.
Discretized for $n$ samples the previous equation can be written as  

\begin{equation}
\begin{bmatrix}
r \\
g \\
b
\end{bmatrix}
= 
\begin{bmatrix}
s_r(\lambda_0) & s_r(\lambda_1) & \cdots & s_r(\lambda_n) \\
s_g(\lambda_0) & s_g(\lambda_1) & \cdots & s_g(\lambda_n) \\
s_b(\lambda_0) & s_b(\lambda_1) & \cdots & s_b(\lambda_n) 
\end{bmatrix}
\begin{bmatrix}
i(\lambda_0) \\
i(\lambda_1) \\
\vdots \\
i(\lambda_n) 
\end{bmatrix}.
\label{eq:spectrum_collapse_disc}
\end{equation}

If the original signal $i(\lambda)$ were to be reconstructed, the physical parameters could be computed using the equations presented in Chapter~\ref{ch:methodology}.
An image rendered with those parameters would closely resemble the original data.
The main challenge in this situation lies in the highly underconstrained nature of the problem, $n$ unknowns in $i(\lambda)$ are to be solved with only three equations from $r$, $g$ and $b$.
Fortunately, several methods have been proposed,~\cite{Smits:1999},~\cite{Sun2001} and~\cite{Drew:2003}, to compute an approximation of $i(\lambda)$ using optimization techniques with a set of prior constrains, such as smooth spectral curves and spatial coherency within the image.

